{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "#import packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  MedHouseVal  \n",
      "0    -122.23        4.526  \n",
      "1    -122.22        3.585  \n",
      "2    -122.24        3.521  \n",
      "3    -122.25        3.413  \n",
      "4    -122.25        3.422  \n"
     ]
    }
   ],
   "source": [
    "#import dataset \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['MedHouseVal'] = data.target #adding outcome variable : value \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "x = df.drop(columns=['MedHouseVal'])\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)  #standardize X \n",
    "y = df['MedHouseVal']\n",
    "# print(x.shape) (20640, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction function\n",
    "def pred(x,w):\n",
    "    return x @ w #matrix multiplication : np.dot(x, w) \n",
    "#initialize\n",
    "r,c = x.shape[0], x.shape[1]\n",
    "iters = 50000\n",
    "alpha = 0.001 #learning rate\n",
    "w = np.ones(c) #initialize weights as 1 or you can try zero : np.zeros(c)\n",
    "loss_hist = np.zeros((0,2))\n",
    "\n",
    "#Sanity check \n",
    "# print(\"x mean/std:\", x.mean(), x.std())\n",
    "# print(\"y mean/std:\", y.mean(), y.std())\n",
    "# print(\"Initial loss:\", np.mean((y - pred(x, w)) ** 2) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 0 loss = 5.499193\n",
      "iter = 100 loss = 4.782777\n",
      "iter = 200 loss = 4.262917\n",
      "iter = 300 loss = 3.881839\n",
      "iter = 400 loss = 3.599438\n",
      "iter = 500 loss = 3.387747\n",
      "iter = 600 loss = 3.227161\n",
      "iter = 700 loss = 3.103855\n",
      "iter = 800 loss = 3.008019\n",
      "iter = 900 loss = 2.932633\n",
      "iter = 1000 loss = 2.872637\n",
      "iter = 1100 loss = 2.824352\n",
      "iter = 1200 loss = 2.785073\n",
      "iter = 1300 loss = 2.752795\n",
      "iter = 1400 loss = 2.726015\n",
      "iter = 1500 loss = 2.703593\n",
      "iter = 1600 loss = 2.684660\n",
      "iter = 1700 loss = 2.668541\n",
      "iter = 1800 loss = 2.654710\n",
      "iter = 1900 loss = 2.642753\n",
      "iter = 2000 loss = 2.632338\n",
      "iter = 2100 loss = 2.623204\n",
      "iter = 2200 loss = 2.615135\n",
      "iter = 2300 loss = 2.607959\n",
      "iter = 2400 loss = 2.601533\n",
      "iter = 2500 loss = 2.595742\n",
      "iter = 2600 loss = 2.590489\n",
      "iter = 2700 loss = 2.585694\n",
      "iter = 2800 loss = 2.581290\n",
      "iter = 2900 loss = 2.577223\n",
      "iter = 3000 loss = 2.573445\n",
      "iter = 3100 loss = 2.569918\n",
      "iter = 3200 loss = 2.566608\n",
      "iter = 3300 loss = 2.563487\n",
      "iter = 3400 loss = 2.560532\n",
      "iter = 3500 loss = 2.557723\n",
      "iter = 3600 loss = 2.555043\n",
      "iter = 3700 loss = 2.552477\n",
      "iter = 3800 loss = 2.550013\n",
      "iter = 3900 loss = 2.547640\n",
      "iter = 4000 loss = 2.545348\n",
      "iter = 4100 loss = 2.543131\n",
      "iter = 4200 loss = 2.540981\n",
      "iter = 4300 loss = 2.538892\n",
      "iter = 4400 loss = 2.536860\n",
      "iter = 4500 loss = 2.534879\n",
      "iter = 4600 loss = 2.532947\n",
      "iter = 4700 loss = 2.531059\n",
      "iter = 4800 loss = 2.529213\n",
      "iter = 4900 loss = 2.527406\n",
      "iter = 5000 loss = 2.525636\n",
      "iter = 5100 loss = 2.523901\n",
      "iter = 5200 loss = 2.522199\n",
      "iter = 5300 loss = 2.520529\n",
      "iter = 5400 loss = 2.518889\n",
      "iter = 5500 loss = 2.517277\n",
      "iter = 5600 loss = 2.515694\n",
      "iter = 5700 loss = 2.514137\n",
      "iter = 5800 loss = 2.512606\n",
      "iter = 5900 loss = 2.511100\n",
      "iter = 6000 loss = 2.509618\n",
      "iter = 6100 loss = 2.508160\n",
      "iter = 6200 loss = 2.506725\n",
      "iter = 6300 loss = 2.505311\n",
      "iter = 6400 loss = 2.503920\n",
      "iter = 6500 loss = 2.502549\n",
      "iter = 6600 loss = 2.501200\n",
      "iter = 6700 loss = 2.499870\n",
      "iter = 6800 loss = 2.498560\n",
      "iter = 6900 loss = 2.497270\n",
      "iter = 7000 loss = 2.495999\n",
      "iter = 7100 loss = 2.494746\n",
      "iter = 7200 loss = 2.493511\n",
      "iter = 7300 loss = 2.492295\n",
      "iter = 7400 loss = 2.491096\n",
      "iter = 7500 loss = 2.489914\n",
      "iter = 7600 loss = 2.488749\n",
      "iter = 7700 loss = 2.487601\n",
      "iter = 7800 loss = 2.486470\n",
      "iter = 7900 loss = 2.485354\n",
      "iter = 8000 loss = 2.484255\n",
      "iter = 8100 loss = 2.483171\n",
      "iter = 8200 loss = 2.482102\n",
      "iter = 8300 loss = 2.481049\n",
      "iter = 8400 loss = 2.480010\n",
      "iter = 8500 loss = 2.478986\n",
      "iter = 8600 loss = 2.477977\n",
      "iter = 8700 loss = 2.476982\n",
      "iter = 8800 loss = 2.476000\n",
      "iter = 8900 loss = 2.475033\n",
      "iter = 9000 loss = 2.474079\n",
      "iter = 9100 loss = 2.473138\n",
      "iter = 9200 loss = 2.472211\n",
      "iter = 9300 loss = 2.471296\n",
      "iter = 9400 loss = 2.470395\n",
      "iter = 9500 loss = 2.469505\n",
      "iter = 9600 loss = 2.468629\n",
      "iter = 9700 loss = 2.467764\n",
      "iter = 9800 loss = 2.466911\n",
      "iter = 9900 loss = 2.466071\n",
      "iter = 10000 loss = 2.465242\n",
      "iter = 10100 loss = 2.464424\n",
      "iter = 10200 loss = 2.463618\n",
      "iter = 10300 loss = 2.462823\n",
      "iter = 10400 loss = 2.462038\n",
      "iter = 10500 loss = 2.461265\n",
      "iter = 10600 loss = 2.460502\n",
      "iter = 10700 loss = 2.459750\n",
      "iter = 10800 loss = 2.459009\n",
      "iter = 10900 loss = 2.458277\n",
      "iter = 11000 loss = 2.457555\n",
      "iter = 11100 loss = 2.456844\n",
      "iter = 11200 loss = 2.456142\n",
      "iter = 11300 loss = 2.455450\n",
      "iter = 11400 loss = 2.454767\n",
      "iter = 11500 loss = 2.454094\n",
      "iter = 11600 loss = 2.453429\n",
      "iter = 11700 loss = 2.452774\n",
      "iter = 11800 loss = 2.452128\n",
      "iter = 11900 loss = 2.451491\n",
      "iter = 12000 loss = 2.450862\n",
      "iter = 12100 loss = 2.450242\n",
      "iter = 12200 loss = 2.449630\n",
      "iter = 12300 loss = 2.449027\n",
      "iter = 12400 loss = 2.448432\n",
      "iter = 12500 loss = 2.447844\n",
      "iter = 12600 loss = 2.447265\n",
      "iter = 12700 loss = 2.446694\n",
      "iter = 12800 loss = 2.446130\n",
      "iter = 12900 loss = 2.445574\n",
      "iter = 13000 loss = 2.445026\n",
      "iter = 13100 loss = 2.444485\n",
      "iter = 13200 loss = 2.443951\n",
      "iter = 13300 loss = 2.443425\n",
      "iter = 13400 loss = 2.442905\n",
      "iter = 13500 loss = 2.442393\n",
      "iter = 13600 loss = 2.441887\n",
      "iter = 13700 loss = 2.441388\n",
      "iter = 13800 loss = 2.440896\n",
      "iter = 13900 loss = 2.440410\n",
      "iter = 14000 loss = 2.439931\n",
      "iter = 14100 loss = 2.439459\n",
      "iter = 14200 loss = 2.438992\n",
      "iter = 14300 loss = 2.438532\n",
      "iter = 14400 loss = 2.438078\n",
      "iter = 14500 loss = 2.437630\n",
      "iter = 14600 loss = 2.437188\n",
      "iter = 14700 loss = 2.436752\n",
      "iter = 14800 loss = 2.436322\n",
      "iter = 14900 loss = 2.435897\n",
      "iter = 15000 loss = 2.435478\n",
      "iter = 15100 loss = 2.435065\n",
      "iter = 15200 loss = 2.434657\n",
      "iter = 15300 loss = 2.434254\n",
      "iter = 15400 loss = 2.433857\n",
      "iter = 15500 loss = 2.433465\n",
      "iter = 15600 loss = 2.433078\n",
      "iter = 15700 loss = 2.432696\n",
      "iter = 15800 loss = 2.432320\n",
      "iter = 15900 loss = 2.431948\n",
      "iter = 16000 loss = 2.431581\n",
      "iter = 16100 loss = 2.431219\n",
      "iter = 16200 loss = 2.430862\n",
      "iter = 16300 loss = 2.430509\n",
      "iter = 16400 loss = 2.430161\n",
      "iter = 16500 loss = 2.429818\n",
      "iter = 16600 loss = 2.429479\n",
      "iter = 16700 loss = 2.429144\n",
      "iter = 16800 loss = 2.428814\n",
      "iter = 16900 loss = 2.428488\n",
      "iter = 17000 loss = 2.428166\n",
      "iter = 17100 loss = 2.427849\n",
      "iter = 17200 loss = 2.427535\n",
      "iter = 17300 loss = 2.427226\n",
      "iter = 17400 loss = 2.426921\n",
      "iter = 17500 loss = 2.426620\n",
      "iter = 17600 loss = 2.426322\n",
      "iter = 17700 loss = 2.426028\n",
      "iter = 17800 loss = 2.425739\n",
      "iter = 17900 loss = 2.425453\n",
      "iter = 18000 loss = 2.425170\n",
      "iter = 18100 loss = 2.424891\n",
      "iter = 18200 loss = 2.424616\n",
      "iter = 18300 loss = 2.424345\n",
      "iter = 18400 loss = 2.424076\n",
      "iter = 18500 loss = 2.423812\n",
      "iter = 18600 loss = 2.423550\n",
      "iter = 18700 loss = 2.423292\n",
      "iter = 18800 loss = 2.423038\n",
      "iter = 18900 loss = 2.422786\n",
      "iter = 19000 loss = 2.422538\n",
      "iter = 19100 loss = 2.422293\n",
      "iter = 19200 loss = 2.422051\n",
      "iter = 19300 loss = 2.421812\n",
      "iter = 19400 loss = 2.421576\n",
      "iter = 19500 loss = 2.421344\n",
      "iter = 19600 loss = 2.421114\n",
      "iter = 19700 loss = 2.420887\n",
      "iter = 19800 loss = 2.420663\n",
      "iter = 19900 loss = 2.420441\n",
      "iter = 20000 loss = 2.420223\n",
      "iter = 20100 loss = 2.420007\n",
      "iter = 20200 loss = 2.419794\n",
      "iter = 20300 loss = 2.419584\n",
      "iter = 20400 loss = 2.419376\n",
      "iter = 20500 loss = 2.419171\n",
      "iter = 20600 loss = 2.418968\n",
      "iter = 20700 loss = 2.418768\n",
      "iter = 20800 loss = 2.418571\n",
      "iter = 20900 loss = 2.418376\n",
      "iter = 21000 loss = 2.418183\n",
      "iter = 21100 loss = 2.417993\n",
      "iter = 21200 loss = 2.417806\n",
      "iter = 21300 loss = 2.417620\n",
      "iter = 21400 loss = 2.417437\n",
      "iter = 21500 loss = 2.417256\n",
      "iter = 21600 loss = 2.417078\n",
      "iter = 21700 loss = 2.416901\n",
      "iter = 21800 loss = 2.416727\n",
      "iter = 21900 loss = 2.416555\n",
      "iter = 22000 loss = 2.416385\n",
      "iter = 22100 loss = 2.416217\n",
      "iter = 22200 loss = 2.416051\n",
      "iter = 22300 loss = 2.415888\n",
      "iter = 22400 loss = 2.415726\n",
      "iter = 22500 loss = 2.415566\n",
      "iter = 22600 loss = 2.415409\n",
      "iter = 22700 loss = 2.415253\n",
      "iter = 22800 loss = 2.415099\n",
      "iter = 22900 loss = 2.414947\n",
      "iter = 23000 loss = 2.414797\n",
      "iter = 23100 loss = 2.414648\n",
      "iter = 23200 loss = 2.414502\n",
      "iter = 23300 loss = 2.414357\n",
      "iter = 23400 loss = 2.414214\n",
      "iter = 23500 loss = 2.414073\n",
      "iter = 23600 loss = 2.413934\n",
      "iter = 23700 loss = 2.413796\n",
      "iter = 23800 loss = 2.413660\n",
      "iter = 23900 loss = 2.413525\n",
      "iter = 24000 loss = 2.413392\n",
      "iter = 24100 loss = 2.413261\n",
      "iter = 24200 loss = 2.413132\n",
      "iter = 24300 loss = 2.413004\n",
      "iter = 24400 loss = 2.412877\n",
      "iter = 24500 loss = 2.412752\n",
      "iter = 24600 loss = 2.412628\n",
      "iter = 24700 loss = 2.412506\n",
      "iter = 24800 loss = 2.412386\n",
      "iter = 24900 loss = 2.412267\n",
      "iter = 25000 loss = 2.412149\n",
      "iter = 25100 loss = 2.412033\n",
      "iter = 25200 loss = 2.411918\n",
      "iter = 25300 loss = 2.411805\n",
      "iter = 25400 loss = 2.411692\n",
      "iter = 25500 loss = 2.411582\n",
      "iter = 25600 loss = 2.411472\n",
      "iter = 25700 loss = 2.411364\n",
      "iter = 25800 loss = 2.411257\n",
      "iter = 25900 loss = 2.411151\n",
      "iter = 26000 loss = 2.411047\n",
      "iter = 26100 loss = 2.410944\n",
      "iter = 26200 loss = 2.410842\n",
      "iter = 26300 loss = 2.410741\n",
      "iter = 26400 loss = 2.410642\n",
      "iter = 26500 loss = 2.410543\n",
      "iter = 26600 loss = 2.410446\n",
      "iter = 26700 loss = 2.410350\n",
      "iter = 26800 loss = 2.410255\n",
      "iter = 26900 loss = 2.410162\n",
      "iter = 27000 loss = 2.410069\n",
      "iter = 27100 loss = 2.409977\n",
      "iter = 27200 loss = 2.409887\n",
      "iter = 27300 loss = 2.409797\n",
      "iter = 27400 loss = 2.409709\n",
      "iter = 27500 loss = 2.409621\n",
      "iter = 27600 loss = 2.409535\n",
      "iter = 27700 loss = 2.409450\n",
      "iter = 27800 loss = 2.409365\n",
      "iter = 27900 loss = 2.409282\n",
      "iter = 28000 loss = 2.409200\n",
      "iter = 28100 loss = 2.409118\n",
      "iter = 28200 loss = 2.409038\n",
      "iter = 28300 loss = 2.408958\n",
      "iter = 28400 loss = 2.408879\n",
      "iter = 28500 loss = 2.408802\n",
      "iter = 28600 loss = 2.408725\n",
      "iter = 28700 loss = 2.408649\n",
      "iter = 28800 loss = 2.408574\n",
      "iter = 28900 loss = 2.408500\n",
      "iter = 29000 loss = 2.408426\n",
      "iter = 29100 loss = 2.408354\n",
      "iter = 29200 loss = 2.408282\n",
      "iter = 29300 loss = 2.408211\n",
      "iter = 29400 loss = 2.408141\n",
      "iter = 29500 loss = 2.408072\n",
      "iter = 29600 loss = 2.408003\n",
      "iter = 29700 loss = 2.407936\n",
      "iter = 29800 loss = 2.407869\n",
      "iter = 29900 loss = 2.407803\n",
      "iter = 30000 loss = 2.407737\n",
      "iter = 30100 loss = 2.407673\n",
      "iter = 30200 loss = 2.407609\n",
      "iter = 30300 loss = 2.407546\n",
      "iter = 30400 loss = 2.407483\n",
      "iter = 30500 loss = 2.407421\n",
      "iter = 30600 loss = 2.407360\n",
      "iter = 30700 loss = 2.407300\n",
      "iter = 30800 loss = 2.407240\n",
      "iter = 30900 loss = 2.407181\n",
      "iter = 31000 loss = 2.407123\n",
      "iter = 31100 loss = 2.407065\n",
      "iter = 31200 loss = 2.407008\n",
      "iter = 31300 loss = 2.406952\n",
      "iter = 31400 loss = 2.406896\n",
      "iter = 31500 loss = 2.406841\n",
      "iter = 31600 loss = 2.406786\n",
      "iter = 31700 loss = 2.406733\n",
      "iter = 31800 loss = 2.406679\n",
      "iter = 31900 loss = 2.406627\n",
      "iter = 32000 loss = 2.406574\n",
      "iter = 32100 loss = 2.406523\n",
      "iter = 32200 loss = 2.406472\n",
      "iter = 32300 loss = 2.406422\n",
      "iter = 32400 loss = 2.406372\n",
      "iter = 32500 loss = 2.406323\n",
      "iter = 32600 loss = 2.406274\n",
      "iter = 32700 loss = 2.406226\n",
      "iter = 32800 loss = 2.406178\n",
      "iter = 32900 loss = 2.406131\n",
      "iter = 33000 loss = 2.406084\n",
      "iter = 33100 loss = 2.406038\n",
      "iter = 33200 loss = 2.405993\n",
      "iter = 33300 loss = 2.405948\n",
      "iter = 33400 loss = 2.405903\n",
      "iter = 33500 loss = 2.405859\n",
      "iter = 33600 loss = 2.405815\n",
      "iter = 33700 loss = 2.405772\n",
      "iter = 33800 loss = 2.405730\n",
      "iter = 33900 loss = 2.405687\n",
      "iter = 34000 loss = 2.405646\n",
      "iter = 34100 loss = 2.405604\n",
      "iter = 34200 loss = 2.405564\n",
      "iter = 34300 loss = 2.405523\n",
      "iter = 34400 loss = 2.405483\n",
      "iter = 34500 loss = 2.405444\n",
      "iter = 34600 loss = 2.405405\n",
      "iter = 34700 loss = 2.405366\n",
      "iter = 34800 loss = 2.405328\n",
      "iter = 34900 loss = 2.405290\n",
      "iter = 35000 loss = 2.405253\n",
      "iter = 35100 loss = 2.405216\n",
      "iter = 35200 loss = 2.405179\n",
      "iter = 35300 loss = 2.405143\n",
      "iter = 35400 loss = 2.405107\n",
      "iter = 35500 loss = 2.405072\n",
      "iter = 35600 loss = 2.405037\n",
      "iter = 35700 loss = 2.405002\n",
      "iter = 35800 loss = 2.404968\n",
      "iter = 35900 loss = 2.404934\n",
      "iter = 36000 loss = 2.404901\n",
      "iter = 36100 loss = 2.404868\n",
      "iter = 36200 loss = 2.404835\n",
      "iter = 36300 loss = 2.404802\n",
      "iter = 36400 loss = 2.404770\n",
      "iter = 36500 loss = 2.404738\n",
      "iter = 36600 loss = 2.404707\n",
      "iter = 36700 loss = 2.404676\n",
      "iter = 36800 loss = 2.404645\n",
      "iter = 36900 loss = 2.404615\n",
      "iter = 37000 loss = 2.404585\n",
      "iter = 37100 loss = 2.404555\n",
      "iter = 37200 loss = 2.404525\n",
      "iter = 37300 loss = 2.404496\n",
      "iter = 37400 loss = 2.404467\n",
      "iter = 37500 loss = 2.404439\n",
      "iter = 37600 loss = 2.404411\n",
      "iter = 37700 loss = 2.404383\n",
      "iter = 37800 loss = 2.404355\n",
      "iter = 37900 loss = 2.404328\n",
      "iter = 38000 loss = 2.404301\n",
      "iter = 38100 loss = 2.404274\n",
      "iter = 38200 loss = 2.404248\n",
      "iter = 38300 loss = 2.404221\n",
      "iter = 38400 loss = 2.404195\n",
      "iter = 38500 loss = 2.404170\n",
      "iter = 38600 loss = 2.404144\n",
      "iter = 38700 loss = 2.404119\n",
      "iter = 38800 loss = 2.404094\n",
      "iter = 38900 loss = 2.404070\n",
      "iter = 39000 loss = 2.404046\n",
      "iter = 39100 loss = 2.404022\n",
      "iter = 39200 loss = 2.403998\n",
      "iter = 39300 loss = 2.403974\n",
      "iter = 39400 loss = 2.403951\n",
      "iter = 39500 loss = 2.403928\n",
      "iter = 39600 loss = 2.403905\n",
      "iter = 39700 loss = 2.403882\n",
      "iter = 39800 loss = 2.403860\n",
      "iter = 39900 loss = 2.403838\n",
      "iter = 40000 loss = 2.403816\n",
      "iter = 40100 loss = 2.403794\n",
      "iter = 40200 loss = 2.403773\n",
      "iter = 40300 loss = 2.403752\n",
      "iter = 40400 loss = 2.403731\n",
      "iter = 40500 loss = 2.403710\n",
      "iter = 40600 loss = 2.403689\n",
      "iter = 40700 loss = 2.403669\n",
      "iter = 40800 loss = 2.403649\n",
      "iter = 40900 loss = 2.403629\n",
      "iter = 41000 loss = 2.403609\n",
      "iter = 41100 loss = 2.403590\n",
      "iter = 41200 loss = 2.403570\n",
      "iter = 41300 loss = 2.403551\n",
      "iter = 41400 loss = 2.403532\n",
      "iter = 41500 loss = 2.403514\n",
      "iter = 41600 loss = 2.403495\n",
      "iter = 41700 loss = 2.403477\n",
      "iter = 41800 loss = 2.403459\n",
      "iter = 41900 loss = 2.403441\n",
      "iter = 42000 loss = 2.403423\n",
      "iter = 42100 loss = 2.403405\n",
      "iter = 42200 loss = 2.403388\n",
      "iter = 42300 loss = 2.403371\n",
      "iter = 42400 loss = 2.403354\n",
      "iter = 42500 loss = 2.403337\n",
      "iter = 42600 loss = 2.403320\n",
      "iter = 42700 loss = 2.403303\n",
      "iter = 42800 loss = 2.403287\n",
      "iter = 42900 loss = 2.403271\n",
      "iter = 43000 loss = 2.403255\n",
      "iter = 43100 loss = 2.403239\n",
      "iter = 43200 loss = 2.403223\n",
      "iter = 43300 loss = 2.403208\n",
      "iter = 43400 loss = 2.403192\n",
      "iter = 43500 loss = 2.403177\n",
      "iter = 43600 loss = 2.403162\n",
      "iter = 43700 loss = 2.403147\n",
      "iter = 43800 loss = 2.403132\n",
      "iter = 43900 loss = 2.403118\n",
      "iter = 44000 loss = 2.403103\n",
      "iter = 44100 loss = 2.403089\n",
      "iter = 44200 loss = 2.403075\n",
      "iter = 44300 loss = 2.403061\n",
      "iter = 44400 loss = 2.403047\n",
      "iter = 44500 loss = 2.403033\n",
      "iter = 44600 loss = 2.403019\n",
      "iter = 44700 loss = 2.403006\n",
      "iter = 44800 loss = 2.402992\n",
      "iter = 44900 loss = 2.402979\n",
      "iter = 45000 loss = 2.402966\n",
      "iter = 45100 loss = 2.402953\n",
      "iter = 45200 loss = 2.402940\n",
      "iter = 45300 loss = 2.402928\n",
      "iter = 45400 loss = 2.402915\n",
      "iter = 45500 loss = 2.402903\n",
      "iter = 45600 loss = 2.402890\n",
      "iter = 45700 loss = 2.402878\n",
      "iter = 45800 loss = 2.402866\n",
      "iter = 45900 loss = 2.402854\n",
      "iter = 46000 loss = 2.402842\n",
      "iter = 46100 loss = 2.402831\n",
      "iter = 46200 loss = 2.402819\n",
      "iter = 46300 loss = 2.402808\n",
      "iter = 46400 loss = 2.402796\n",
      "iter = 46500 loss = 2.402785\n",
      "iter = 46600 loss = 2.402774\n",
      "iter = 46700 loss = 2.402763\n",
      "iter = 46800 loss = 2.402752\n",
      "iter = 46900 loss = 2.402741\n",
      "iter = 47000 loss = 2.402730\n",
      "iter = 47100 loss = 2.402720\n",
      "iter = 47200 loss = 2.402709\n",
      "iter = 47300 loss = 2.402699\n",
      "iter = 47400 loss = 2.402689\n",
      "iter = 47500 loss = 2.402679\n",
      "iter = 47600 loss = 2.402669\n",
      "iter = 47700 loss = 2.402659\n",
      "iter = 47800 loss = 2.402649\n",
      "iter = 47900 loss = 2.402639\n",
      "iter = 48000 loss = 2.402629\n",
      "iter = 48100 loss = 2.402620\n",
      "iter = 48200 loss = 2.402610\n",
      "iter = 48300 loss = 2.402601\n",
      "iter = 48400 loss = 2.402591\n",
      "iter = 48500 loss = 2.402582\n",
      "iter = 48600 loss = 2.402573\n",
      "iter = 48700 loss = 2.402564\n",
      "iter = 48800 loss = 2.402555\n",
      "iter = 48900 loss = 2.402546\n",
      "iter = 49000 loss = 2.402538\n",
      "iter = 49100 loss = 2.402529\n",
      "iter = 49200 loss = 2.402520\n",
      "iter = 49300 loss = 2.402512\n",
      "iter = 49400 loss = 2.402503\n",
      "iter = 49500 loss = 2.402495\n",
      "iter = 49600 loss = 2.402487\n",
      "iter = 49700 loss = 2.402479\n",
      "iter = 49800 loss = 2.402470\n",
      "iter = 49900 loss = 2.402462\n",
      "initial loss : 5.499193 \n",
      "final loss : 2.402462 \n"
     ]
    }
   ],
   "source": [
    "for k in range(iters):\n",
    "    y_pred = pred(x, w)              # prediction values\n",
    "\n",
    "    # gradient descent update using the error\n",
    "    gradient = x.T @ (y_pred - y) #derivatibe of MSE \n",
    "    w -= alpha * gradient / r  # or w -= alpha * grad, depending on sign convention\n",
    "\n",
    "    # for every 100 iterations, compute loss\n",
    "    if k % 100 == 0:\n",
    "        y_diff = y - pred(x, w)\n",
    "        loss = np.mean(y_diff ** 2) / 2\n",
    "        loss_hist = np.vstack((loss_hist, [k, loss]))\n",
    "        print(\"iter = %d loss = %f\" % (k, loss))\n",
    "\n",
    "print('initial loss : %f ' % loss_hist[0,1])\n",
    "print('final loss : %f ' % loss_hist[-1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist[:,0],loss_hist[:,1])\n",
    "plt.show() # loss curve: model converges quickly (Try different learning rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
